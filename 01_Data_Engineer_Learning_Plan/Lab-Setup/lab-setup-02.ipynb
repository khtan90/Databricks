{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87ace04-e2c8-4a08-b598-d7bbf781b915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/kianhow2000@gmail.com/Databricks/01_Data_Engineer_Learning_Plan/Lab-Setup/common-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70a4595-f60b-4c7c-9a4b-8f8852e2ff72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# Setup for Auto Loader CSV Labs\n",
    "# ==========================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA_PREFIX = \"data_engineering_labs\"\n",
    "VOLUME = \"v01\"\n",
    "\n",
    "RUN_ID = get_run_id()                # e.g. \"00\"\n",
    "SCHEMA = f\"{SCHEMA_PREFIX}_{RUN_ID}\" # data_engineering_labs_00\n",
    "\n",
    "SOURCE_ROWS = 3150\n",
    "STAGING_ROWS_1 = 1000\n",
    "STAGING_ROWS_2 = 2000\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "banner(\"Setting up Auto Loader CSV datasets\")\n",
    "\n",
    "ensure_catalog_and_schema(CATALOG, SCHEMA)\n",
    "VOLUME_ROOT = ensure_volume(CATALOG, SCHEMA, VOLUME)\n",
    "\n",
    "SOURCE_DIR = f\"{VOLUME_ROOT}/csv_files_autoloader_source\"\n",
    "STAGING_DIR = f\"{VOLUME_ROOT}/csv_files_autoloader_staging\"\n",
    "\n",
    "dbutils.fs.mkdirs(SOURCE_DIR)\n",
    "dbutils.fs.mkdirs(STAGING_DIR)\n",
    "\n",
    "print(\"Source dir :\", SOURCE_DIR)\n",
    "print(\"Staging dir:\", STAGING_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Helper to generate items JSON\n",
    "# -----------------------------\n",
    "ITEM_TEMPLATES = [\n",
    "    {\n",
    "        \"coupon\": \"NEWBED10\",\n",
    "        \"item_id\": \"M_STAN_F\",\n",
    "        \"item_name\": \"standard full mattress\",\n",
    "        \"item_revenue_in_usd\": \"850.3\",\n",
    "        \"price\": \"849\"\n",
    "    },\n",
    "    {\n",
    "        \"coupon\": \"SLEEP5\",\n",
    "        \"item_id\": \"P_BASIC\",\n",
    "        \"item_name\": \"basic pillow\",\n",
    "        \"item_revenue_in_usd\": \"55.0\",\n",
    "        \"price\": \"59\"\n",
    "    },\n",
    "    {\n",
    "        \"coupon\": None,\n",
    "        \"item_id\": \"B_FRAME_Q\",\n",
    "        \"item_name\": \"queen bed frame\",\n",
    "        \"item_revenue_in_usd\": \"399.0\",\n",
    "        \"price\": \"399\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def random_items():\n",
    "    n = random.randint(1, 3)\n",
    "    return json.dumps(random.sample(ITEM_TEMPLATES, n))\n",
    "\n",
    "# -----------------------------\n",
    "# Base dataframe generator\n",
    "# -----------------------------\n",
    "def generate_orders_df(start_id, rows):\n",
    "    return (\n",
    "        spark.range(start_id, start_id + rows)\n",
    "        .withColumnRenamed(\"id\", \"order_id\")\n",
    "        .withColumn(\"email\", F.concat(F.lit(\"user\"), F.col(\"order_id\"), F.lit(\"@example.com\")))\n",
    "        .withColumn(\n",
    "            \"transactions_timestamp\",\n",
    "            F.expr(\"timestampadd(SECOND, cast(rand()*86400 as int), current_timestamp())\")\n",
    "        )\n",
    "        .withColumn(\"total_item_quantity\", (F.rand() * 5 + 1).cast(\"int\"))\n",
    "        .withColumn(\"purchase_revenue_in_usd\", F.round(F.rand() * 1200 + 20, 2))\n",
    "        .withColumn(\"unique_items\", (F.rand() * 3 + 1).cast(\"int\"))\n",
    "        .withColumn(\"items\", F.udf(lambda: random_items(), \"string\")())\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"email\",\n",
    "            \"transactions_timestamp\",\n",
    "            \"total_item_quantity\",\n",
    "            \"purchase_revenue_in_usd\",\n",
    "            \"unique_items\",\n",
    "            \"items\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Write 000.csv (3150 rows)\n",
    "# -----------------------------\n",
    "df_000 = generate_orders_df(1, SOURCE_ROWS)\n",
    "\n",
    "(df_000\n",
    " .coalesce(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(f\"{SOURCE_DIR}/000_tmp\"))\n",
    "\n",
    "# Rename part file → 000.csv\n",
    "src_file = [f.path for f in dbutils.fs.ls(f\"{SOURCE_DIR}/000_tmp\") if f.name.startswith(\"part-\")][0]\n",
    "dbutils.fs.mv(src_file, f\"{SOURCE_DIR}/000.csv\", True)\n",
    "dbutils.fs.rm(f\"{SOURCE_DIR}/000_tmp\", True)\n",
    "\n",
    "# -----------------------------\n",
    "# Write 001.csv (1000 rows)\n",
    "# -----------------------------\n",
    "df_001 = generate_orders_df(10_000, STAGING_ROWS_1)\n",
    "\n",
    "(df_001\n",
    " .coalesce(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(f\"{STAGING_DIR}/001_tmp\"))\n",
    "\n",
    "src_file = [f.path for f in dbutils.fs.ls(f\"{STAGING_DIR}/001_tmp\") if f.name.startswith(\"part-\")][0]\n",
    "dbutils.fs.mv(src_file, f\"{STAGING_DIR}/001.csv\", True)\n",
    "dbutils.fs.rm(f\"{STAGING_DIR}/001_tmp\", True)\n",
    "\n",
    "# -----------------------------\n",
    "# Write 002.csv (2000 rows)\n",
    "# -----------------------------\n",
    "df_002 = generate_orders_df(20_000, STAGING_ROWS_2)\n",
    "\n",
    "(df_002\n",
    " .coalesce(1)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(f\"{STAGING_DIR}/002_tmp\"))\n",
    "\n",
    "src_file = [f.path for f in dbutils.fs.ls(f\"{STAGING_DIR}/002_tmp\") if f.name.startswith(\"part-\")][0]\n",
    "dbutils.fs.mv(src_file, f\"{STAGING_DIR}/002.csv\", True)\n",
    "dbutils.fs.rm(f\"{STAGING_DIR}/002_tmp\", True)\n",
    "\n",
    "# -----------------------------\n",
    "# Final verification\n",
    "# -----------------------------\n",
    "print(\"Source files:\", [f.name for f in dbutils.fs.ls(SOURCE_DIR)])\n",
    "print(\"Staging files:\", [f.name for f in dbutils.fs.ls(STAGING_DIR)])\n",
    "\n",
    "print(\"000.csv rows :\", spark.read.option(\"header\",\"true\").csv(f\"{SOURCE_DIR}/000.csv\").count())\n",
    "print(\"001.csv rows :\", spark.read.option(\"header\",\"true\").csv(f\"{STAGING_DIR}/001.csv\").count())\n",
    "print(\"002.csv rows :\", spark.read.option(\"header\",\"true\").csv(f\"{STAGING_DIR}/002.csv\").count())\n",
    "\n",
    "print(\"✅ Auto Loader CSV setup complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab-setup-02",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
