{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9fe2b9-bdcc-452c-9f8d-5ebdd4372898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/kianhow2000@gmail.com/Databricks/01_Data_Engineer_Learning_Plan/Lab-Setup/common-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b4bfca6-d284-411d-9231-94a5214fb53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# -----------------------------\n",
    "# Target paths\n",
    "# -----------------------------\n",
    "ORDERS_FILE   = \"/Volumes/workspace/data_engineering_labs_00/v01/orders/00.json\"\n",
    "STATUS_FILE   = \"/Volumes/workspace/data_engineering_labs_00/v01/status/00.json\"\n",
    "CUSTOMERS_FILE = \"/Volumes/workspace/data_engineering_labs_00/v01/customers/00.json\"\n",
    "CUSTOMERS_NEW_FILE = \"/Volumes/workspace/data_engineering_labs_00/v01/customers_new/01.json\"\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def banner(title):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(title)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def ensure_parent_dir(file_path: str):\n",
    "    parent = file_path.rsplit(\"/\", 1)[0]\n",
    "    dbutils.fs.mkdirs(parent)\n",
    "    return parent\n",
    "\n",
    "def write_single_json_file(df, final_file_path: str, overwrite: bool = True):\n",
    "    \"\"\"\n",
    "    Spark JSON writer writes directories. This helper:\n",
    "      1) writes df as a single part file to a temp directory\n",
    "      2) moves the part-*.json to the desired final filename\n",
    "      3) removes temp directory\n",
    "    \"\"\"\n",
    "    ensure_parent_dir(final_file_path)\n",
    "\n",
    "    tmp_dir = final_file_path + \".__tmp__\"\n",
    "    mode = \"overwrite\" if overwrite else \"error\"\n",
    "\n",
    "    # Clean tmp if it exists\n",
    "    try:\n",
    "        dbutils.fs.rm(tmp_dir, recurse=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    (df.coalesce(1)\n",
    "       .write\n",
    "       .mode(mode)\n",
    "       .json(tmp_dir))\n",
    "\n",
    "    # Find the single part file\n",
    "    part_files = [f.path for f in dbutils.fs.ls(tmp_dir) if f.path.endswith(\".json\")]\n",
    "    if len(part_files) != 1:\n",
    "        raise Exception(f\"Expected exactly 1 part json in {tmp_dir}, found {len(part_files)}: {part_files}\")\n",
    "\n",
    "    part_path = part_files[0]\n",
    "\n",
    "    # Remove final if exists (overwrite semantics)\n",
    "    try:\n",
    "        dbutils.fs.rm(final_file_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dbutils.fs.mv(part_path, final_file_path)\n",
    "\n",
    "    # Remove temp dir (contains _SUCCESS and maybe crc files)\n",
    "    dbutils.fs.rm(tmp_dir, recurse=True)\n",
    "\n",
    "def assert_count(df, expected: int, name: str):\n",
    "    c = df.count()\n",
    "    if c != expected:\n",
    "        raise Exception(f\"{name}: expected {expected} rows, got {c}\")\n",
    "    print(f\"✅ {name}: {c} rows\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset 1: orders (174 rows)\n",
    "# -----------------------------\n",
    "banner(\"Creating Dataset 1: orders/00.json (174 rows)\")\n",
    "\n",
    "ORDERS_N = 174\n",
    "\n",
    "# Make order_id such that sample 75123 exists (when idx=124 -> 75000+123 = 75123)\n",
    "ORDER_ID_BASE = 75000\n",
    "\n",
    "# Make customer_id range align with customers dataset (see below), and allow sample 23564 to exist:\n",
    "# customers will be customer_id = 23000 + customer_idx, so 23564 exists when customer_idx=564.\n",
    "CUSTOMER_ID_BASE = 23000\n",
    "\n",
    "# Use an epoch seconds base around the provided sample scale\n",
    "ORDER_TS_BASE = 1640390000  # close to the sample 1640394862\n",
    "\n",
    "orders_df = (\n",
    "    spark.range(1, ORDERS_N + 1).withColumnRenamed(\"id\", \"idx\")\n",
    "      .withColumn(\"order_id\", (F.lit(ORDER_ID_BASE) + F.col(\"idx\") - F.lit(1)).cast(\"long\"))\n",
    "      .withColumn(\n",
    "          \"customer_id\",\n",
    "          (F.lit(CUSTOMER_ID_BASE) +\n",
    "           F.pmod(F.xxhash64(F.col(\"idx\"), F.lit(SEED)), F.lit(1000))).cast(\"long\")\n",
    "      )\n",
    "      .withColumn(\n",
    "          \"notifications\",\n",
    "          F.when(F.pmod(F.xxhash64(F.col(\"order_id\"), F.lit(SEED)), F.lit(2)) == 0, F.lit(\"Y\"))\n",
    "           .otherwise(F.lit(\"N\"))\n",
    "      )\n",
    "      .withColumn(\n",
    "          \"order_timestamp\",\n",
    "          (F.lit(ORDER_TS_BASE) +\n",
    "           F.pmod(F.xxhash64(F.col(\"order_id\"), F.lit(SEED)), F.lit(30 * 24 * 60 * 60))).cast(\"long\")\n",
    "      )\n",
    "      .select(\"customer_id\", \"notifications\", \"order_id\", \"order_timestamp\")\n",
    ")\n",
    "\n",
    "# Ensure notifications only Y/N\n",
    "bad_notif = orders_df.filter(~F.col(\"notifications\").isin(\"Y\", \"N\")).count()\n",
    "if bad_notif != 0:\n",
    "    raise Exception(f\"Found {bad_notif} invalid notifications values\")\n",
    "\n",
    "assert_count(orders_df, ORDERS_N, \"orders_df\")\n",
    "\n",
    "write_single_json_file(orders_df, ORDERS_FILE)\n",
    "print(\"✅ Written:\", ORDERS_FILE)\n",
    "\n",
    "# Optional: show a couple of rows (including sample-like scale)\n",
    "display(orders_df.orderBy(\"order_id\").limit(5))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset 2: status (5000 rows)\n",
    "# -----------------------------\n",
    "banner(\"Creating Dataset 2: status/00.json (5000 rows)\")\n",
    "\n",
    "STATUS_N = 5000\n",
    "\n",
    "status_values = [\n",
    "    \"on the way\",\n",
    "    \"cancelled\",\n",
    "    \"return cancelled\",\n",
    "    \"report shipping error\",\n",
    "    \"delivered\",\n",
    "    \"return processed\",\n",
    "    \"return picked up\",\n",
    "    \"placed\",\n",
    "    \"preparing\",\n",
    "    \"return requested\"\n",
    "]\n",
    "\n",
    "# We'll cycle through the 174 order_ids so ALL 5000 rows can join to orders on order_id\n",
    "# order_id = ORDER_ID_BASE + (id % ORDERS_N)\n",
    "status_df = (\n",
    "    spark.range(0, STATUS_N).withColumnRenamed(\"id\", \"idx\")\n",
    "      .withColumn(\"order_id\", (F.lit(ORDER_ID_BASE) + F.pmod(F.col(\"idx\"), F.lit(ORDERS_N))).cast(\"long\"))\n",
    "      .withColumn(\n",
    "          \"order_status\",\n",
    "          F.element_at(\n",
    "              F.array(*[F.lit(s) for s in status_values]),\n",
    "              (F.pmod(F.xxhash64(F.col(\"idx\"), F.lit(SEED)), F.lit(len(status_values))) + F.lit(1)).cast(\"int\")\n",
    "          )\n",
    "      )\n",
    "      .withColumn(\n",
    "          \"status_timestamp\",\n",
    "          # timestamps near sample 1640392092; spread within ~60 days\n",
    "          (F.lit(ORDER_TS_BASE) +\n",
    "           F.pmod(F.xxhash64(F.col(\"order_id\"), F.col(\"idx\"), F.lit(SEED)), F.lit(60 * 24 * 60 * 60))).cast(\"long\")\n",
    "      )\n",
    "      .select(\"order_id\", \"order_status\", \"status_timestamp\")\n",
    ")\n",
    "\n",
    "assert_count(status_df, STATUS_N, \"status_df\")\n",
    "\n",
    "# Validate join coverage: every status row should match an order_id\n",
    "status_unmatched = (\n",
    "    status_df.join(orders_df.select(\"order_id\").distinct(), on=\"order_id\", how=\"left_anti\").count()\n",
    ")\n",
    "if status_unmatched != 0:\n",
    "    raise Exception(f\"Expected all status rows to match orders.order_id; unmatched={status_unmatched}\")\n",
    "print(\"✅ status_df: all rows joinable to orders on order_id\")\n",
    "\n",
    "write_single_json_file(status_df, STATUS_FILE)\n",
    "print(\"✅ Written:\", STATUS_FILE)\n",
    "\n",
    "display(status_df.limit(5))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset 3: customers (1000 rows)\n",
    "# -----------------------------\n",
    "banner(\"Creating Dataset 3: customers/00.json (1000 rows)\")\n",
    "\n",
    "CUSTOMERS_N = 1000\n",
    "\n",
    "cities = [\"Singapore\", \"Jurong\", \"Tampines\", \"Woodlands\", \"Hougang\", \"Bedok\", \"Bukit Batok\", \"Ang Mo Kio\"]\n",
    "\n",
    "customers_df = (\n",
    "    spark.range(1, CUSTOMERS_N + 1).withColumnRenamed(\"id\", \"idx\")\n",
    "      .withColumn(\"customer_id\", (F.lit(CUSTOMER_ID_BASE) + F.col(\"idx\") - F.lit(1)).cast(\"long\"))\n",
    "      .withColumn(\"name\", F.concat(F.lit(\"Customer \"), F.col(\"customer_id\").cast(\"string\")))\n",
    "      .withColumn(\"email\", F.concat(F.lit(\"customer\"), F.col(\"customer_id\").cast(\"string\"), F.lit(\"@example.com\")))\n",
    "      .withColumn(\n",
    "          \"city\",\n",
    "          F.element_at(\n",
    "              F.array(*[F.lit(c) for c in cities]),\n",
    "              (F.pmod(F.xxhash64(F.col(\"customer_id\"), F.lit(SEED)), F.lit(len(cities))) + F.lit(1)).cast(\"int\")\n",
    "          )\n",
    "      )\n",
    "      .withColumn(\"address\", F.concat(F.lit(\"Blk \"), F.pmod(F.col(\"customer_id\"), F.lit(999)).cast(\"string\"),\n",
    "                                      F.lit(\", Street \"), F.pmod(F.col(\"customer_id\"), F.lit(50)).cast(\"string\")))\n",
    "      .withColumn(\"operation\", F.lit(\"NEW\"))\n",
    "      .select(\"address\", \"city\", \"customer_id\", \"email\", \"name\", \"operation\")\n",
    ")\n",
    "\n",
    "assert_count(customers_df, CUSTOMERS_N, \"customers_df\")\n",
    "\n",
    "# Validate operation all NEW\n",
    "bad_ops = customers_df.filter(F.col(\"operation\") != \"NEW\").count()\n",
    "if bad_ops != 0:\n",
    "    raise Exception(f\"Expected all customers.operation to be NEW; bad={bad_ops}\")\n",
    "\n",
    "write_single_json_file(customers_df, CUSTOMERS_FILE)\n",
    "print(\"✅ Written:\", CUSTOMERS_FILE)\n",
    "\n",
    "display(customers_df.limit(5))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset 4: customers_new (23 rows)\n",
    "# -----------------------------\n",
    "banner(\"Creating Dataset 4: customers_new/01.json (23 rows)\")\n",
    "\n",
    "# Requirements:\n",
    "# - 23 records\n",
    "# - same columns as dataset 3\n",
    "# - operation distribution: 12 UPDATE, 1 DELETE, 10 NEW\n",
    "# - UPDATE: record matches dataset 3 exactly EXCEPT email changed\n",
    "# - DELETE: customer_id found in dataset 3, all other values null\n",
    "# - NEW: new generated customers\n",
    "\n",
    "UPDATES_N = 12\n",
    "DELETES_N = 1\n",
    "NEWS_N = 10\n",
    "\n",
    "# Pick deterministic customer_ids for update/delete from existing customers\n",
    "# Use ordering by hash for repeatability\n",
    "existing_ids_df = (\n",
    "    customers_df\n",
    "      .select(\"customer_id\")\n",
    "      .withColumn(\"h\", F.xxhash64(F.col(\"customer_id\"), F.lit(SEED)))\n",
    "      .orderBy(\"h\")\n",
    ")\n",
    "\n",
    "update_ids = [r[\"customer_id\"] for r in existing_ids_df.limit(UPDATES_N).collect()]\n",
    "delete_id  = existing_ids_df.filter(~F.col(\"customer_id\").isin(update_ids)).limit(1).collect()[0][\"customer_id\"]\n",
    "\n",
    "updates_df = (\n",
    "    customers_df\n",
    "      .filter(F.col(\"customer_id\").isin(update_ids))\n",
    "      .withColumn(\"email\", F.concat(F.lit(\"updated_\"), F.col(\"customer_id\").cast(\"string\"), F.lit(\"@example.com\")))\n",
    "      .withColumn(\"operation\", F.lit(\"UPDATE\"))\n",
    ")\n",
    "\n",
    "\n",
    "delete_df = (\n",
    "    spark.range(1).select(\n",
    "        F.lit(None).cast(\"string\").alias(\"address\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"city\"),\n",
    "        F.lit(int(delete_id)).cast(\"long\").alias(\"customer_id\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"email\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"name\"),\n",
    "        F.lit(\"DELETE\").alias(\"operation\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# New customer_ids outside dataset 3 range to avoid collisions\n",
    "# customers in dataset 3 are 23000..23999\n",
    "new_base = 50000\n",
    "new_customers_df = (\n",
    "    spark.range(1, NEWS_N + 1).withColumnRenamed(\"id\", \"idx\")\n",
    "      .withColumn(\"customer_id\", (F.lit(new_base) + F.col(\"idx\")).cast(\"long\"))\n",
    "      .withColumn(\"name\", F.concat(F.lit(\"New Customer \"), F.col(\"customer_id\").cast(\"string\")))\n",
    "      .withColumn(\"email\", F.concat(F.lit(\"newcustomer\"), F.col(\"customer_id\").cast(\"string\"), F.lit(\"@example.com\")))\n",
    "      .withColumn(\n",
    "          \"city\",\n",
    "          F.element_at(\n",
    "              F.array(*[F.lit(c) for c in cities]),\n",
    "              (F.pmod(F.xxhash64(F.col(\"customer_id\"), F.lit(SEED)), F.lit(len(cities))) + F.lit(1)).cast(\"int\")\n",
    "          )\n",
    "      )\n",
    "      .withColumn(\"address\", F.concat(F.lit(\"Unit \"), F.pmod(F.col(\"customer_id\"), F.lit(200)).cast(\"string\"),\n",
    "                                      F.lit(\", Avenue \"), F.pmod(F.col(\"customer_id\"), F.lit(30)).cast(\"string\")))\n",
    "      .withColumn(\"operation\", F.lit(\"NEW\"))\n",
    "      .select(\"address\", \"city\", \"customer_id\", \"email\", \"name\", \"operation\")\n",
    ")\n",
    "\n",
    "customers_new_df = updates_df.unionByName(delete_df).unionByName(new_customers_df)\n",
    "\n",
    "# Validations\n",
    "assert_count(customers_new_df, UPDATES_N + DELETES_N + NEWS_N, \"customers_new_df\")\n",
    "\n",
    "ops_check = customers_new_df.groupBy(\"operation\").count()\n",
    "display(ops_check)\n",
    "\n",
    "# Confirm 12 UPDATE, 1 DELETE, 10 NEW\n",
    "op_counts = {r[\"operation\"]: r[\"count\"] for r in ops_check.collect()}\n",
    "if op_counts.get(\"UPDATE\", 0) != UPDATES_N or op_counts.get(\"DELETE\", 0) != DELETES_N or op_counts.get(\"NEW\", 0) != NEWS_N:\n",
    "    raise Exception(f\"Operation counts wrong: {op_counts}\")\n",
    "\n",
    "# Confirm UPDATE rows match dataset 3 except email\n",
    "# Join on customer_id and compare columns\n",
    "cmp = (\n",
    "    customers_new_df.filter(F.col(\"operation\") == \"UPDATE\").alias(\"n\")\n",
    "      .join(customers_df.alias(\"c\"), on=\"customer_id\", how=\"inner\")\n",
    "      .select(\n",
    "          \"customer_id\",\n",
    "          (F.col(\"n.address\") == F.col(\"c.address\")).alias(\"address_same\"),\n",
    "          (F.col(\"n.city\") == F.col(\"c.city\")).alias(\"city_same\"),\n",
    "          (F.col(\"n.name\") == F.col(\"c.name\")).alias(\"name_same\"),\n",
    "          (F.col(\"n.email\") == F.col(\"c.email\")).alias(\"email_same\"),\n",
    "      )\n",
    ")\n",
    "bad_update = cmp.filter(~(F.col(\"address_same\") & F.col(\"city_same\") & F.col(\"name_same\")) | (F.col(\"email_same\"))).count()\n",
    "if bad_update != 0:\n",
    "    raise Exception(f\"UPDATE validation failed: {bad_update} rows did not match rules\")\n",
    "print(\"✅ UPDATE validation passed (matches dataset 3 except email changed)\")\n",
    "\n",
    "# Confirm DELETE row has nulls except customer_id + operation\n",
    "del_bad = customers_new_df.filter(F.col(\"operation\") == \"DELETE\") \\\n",
    "    .filter(~(F.col(\"address\").isNull() & F.col(\"city\").isNull() & F.col(\"email\").isNull() & F.col(\"name\").isNull())).count()\n",
    "if del_bad != 0:\n",
    "    raise Exception(\"DELETE validation failed: non-null fields found in DELETE row\")\n",
    "print(\"✅ DELETE validation passed (nulls except customer_id)\")\n",
    "\n",
    "write_single_json_file(customers_new_df, CUSTOMERS_NEW_FILE)\n",
    "print(\"✅ Written:\", CUSTOMERS_NEW_FILE)\n",
    "\n",
    "display(customers_new_df.orderBy(\"operation\", \"customer_id\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab-setup-06",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
