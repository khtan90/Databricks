{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87ace04-e2c8-4a08-b598-d7bbf781b915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/kianhow2000@gmail.com/Databricks/01_Data_Engineer_Learning_Plan/Lab-Setup/common-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70a4595-f60b-4c7c-9a4b-8f8852e2ff72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# -----------------------------\n",
    "# Lab-specific configuration\n",
    "# -----------------------------\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA_PREFIX = \"data_engineering_labs\"\n",
    "VOLUME = \"v01\"\n",
    "\n",
    "TOTAL_RECORDS = 10_000\n",
    "NUM_PART_FILES = 4\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Resolve run context\n",
    "# -----------------------------\n",
    "RUN_ID = get_run_id()\n",
    "SCHEMA = f\"{SCHEMA_PREFIX}_{RUN_ID}\"\n",
    "\n",
    "banner(f\"Setting up Lab 01 in {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "ensure_catalog_and_schema(CATALOG, SCHEMA)\n",
    "VOLUME_ROOT = ensure_volume(CATALOG, SCHEMA, VOLUME)\n",
    "\n",
    "OUT_DIR = f\"{VOLUME_ROOT}/raw/users-historical\"\n",
    "dbutils.fs.mkdirs(OUT_DIR)\n",
    "\n",
    "print(\"Output directory:\", OUT_DIR)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate 10,000 records\n",
    "# -----------------------------\n",
    "# user_id: 1..10000\n",
    "# user_first_touch_timestamp: BIGINT epoch millis (portable / common in labs)\n",
    "# email: user<id>@example.com\n",
    "\n",
    "# Fixed base time: 2024-01-01 00:00:00 UTC in epoch millis\n",
    "BASE_EPOCH_MS = 1704067200000\n",
    "\n",
    "df = (spark.range(1, TOTAL_RECORDS + 1)\n",
    "      .withColumnRenamed(\"id\", \"user_id\")\n",
    "      # Add a deterministic pseudo-random offset (0..~365 days in ms) per user\n",
    "      .withColumn(\n",
    "          \"user_first_touch_timestamp\",\n",
    "          (F.lit(BASE_EPOCH_MS) +\n",
    "           (F.pmod(F.xxhash64(F.col(\"user_id\"), F.lit(SEED)), F.lit(365 * 24 * 60 * 60 * 1000))).cast(\"bigint\"))\n",
    "      )\n",
    "      .withColumn(\"email\", F.concat(F.lit(\"user\"), F.col(\"user_id\"), F.lit(\"@example.com\")))\n",
    "      .select(\"user_id\", \"user_first_touch_timestamp\", \"email\")\n",
    ")\n",
    "\n",
    "# Sanity check row count before write\n",
    "count_before = df.count()\n",
    "if count_before != TOTAL_RECORDS:\n",
    "    raise Exception(f\"Expected {TOTAL_RECORDS} rows, got {count_before}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Write Parquet with exactly 4 part files\n",
    "# -----------------------------\n",
    "# repartition(NUM_PART_FILES) ensures 4 output part-xxxxx files\n",
    "(df.repartition(NUM_PART_FILES)\n",
    "   .write\n",
    "   .mode(\"overwrite\")\n",
    "   .parquet(OUT_DIR))\n",
    "\n",
    "# -----------------------------\n",
    "# Verify outputs: 4 parquet part files, and 10,000 records on read\n",
    "# -----------------------------\n",
    "files = [f.path for f in dbutils.fs.ls(OUT_DIR) if f.path.endswith(\".parquet\")]\n",
    "files_sorted = sorted(files)\n",
    "\n",
    "print(\"Parquet files written:\")\n",
    "for p in files_sorted:\n",
    "    print(\" -\", p.split(\"/\")[-1])\n",
    "\n",
    "if len(files_sorted) != NUM_PART_FILES:\n",
    "    raise Exception(f\"Expected {NUM_PART_FILES} parquet files, found {len(files_sorted)}\")\n",
    "\n",
    "# Check expected prefix pattern part-00000, part-00001, ...\n",
    "# Spark will typically name them like part-00000-<uuid>.snappy.parquet\n",
    "for i, p in enumerate(files_sorted):\n",
    "    expected_prefix = f\"part-{i:05d}\"\n",
    "    actual_name = p.split(\"/\")[-1]\n",
    "    if not actual_name.startswith(expected_prefix):\n",
    "        raise Exception(f\"Expected file starting with {expected_prefix}, got {actual_name}\")\n",
    "\n",
    "# Verify record count by reading back\n",
    "read_back = spark.read.parquet(OUT_DIR)\n",
    "count_after = read_back.count()\n",
    "if count_after != TOTAL_RECORDS:\n",
    "    raise Exception(f\"After write: expected {TOTAL_RECORDS} rows, got {count_after}\")\n",
    "\n",
    "print(\"âœ… Setup complete.\")\n",
    "print(f\"Dataset location: {OUT_DIR}\")\n",
    "print(\"Schema:\")\n",
    "read_back.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab-setup-01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
