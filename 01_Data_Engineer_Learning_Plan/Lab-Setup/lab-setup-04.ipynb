{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d085fbc-1dd9-4713-9e8b-52c3c29384b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run /Workspace/Users/kianhow2000@gmail.com/Databricks/01_Data_Engineer_Learning_Plan/Lab-Setup/common-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff4917e6-3d6a-4fab-9fd1-69ee3ac23ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "### Setup:\n",
    "### -  sales CSV + malformed CSV demo files (Unity Catalog)\n",
    "### Creates:\n",
    "### - 1. /Volumes/workspace/data_engineering_labs_<run_id>/v01/raw/sales-csv/000.csv..003.csv (1500 rows each)\n",
    "### - 2. /Volumes/workspace/data_engineering_labs_<run_id>/v01/ops/csv_demo_files/malformed_example_1_data.csv\n",
    "### - 3. /Volumes/workspace/data_engineering_labs_<run_id>/v01/ops/csv_demo_files/malformed_example_2_data.csv\n",
    "### - IMPORTANT: absolute path to your common-utils\n",
    "### `%run /Workspace/Users/kianhow2000@gmail.com/Databricks/01_Data_Engineer_Learning_Plan/Lab-Setup/common-utils`\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA_PREFIX = \"data_engineering_labs\"\n",
    "VOLUME = \"v01\"\n",
    "\n",
    "RUN_ID = get_run_id(default=\"00\")             \n",
    "SCHEMA = f\"{SCHEMA_PREFIX}_{RUN_ID}\"           \n",
    "\n",
    "ROWS_PER_FILE = 1500\n",
    "NUM_SALES_FILES = 4\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "banner(f\"Setting up SALES CSV + malformed demos in {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "ensure_catalog_and_schema(CATALOG, SCHEMA)\n",
    "VOLUME_ROOT = ensure_volume(CATALOG, SCHEMA, VOLUME)\n",
    "\n",
    "RAW_SALES_DIR = f\"{VOLUME_ROOT}/raw/sales-csv\"\n",
    "OPS_DEMO_DIR  = f\"{VOLUME_ROOT}/ops/csv_demo_files\"\n",
    "\n",
    "dbutils.fs.mkdirs(RAW_SALES_DIR)\n",
    "dbutils.fs.mkdirs(OPS_DEMO_DIR)\n",
    "\n",
    "print(\"RAW sales dir:\", RAW_SALES_DIR)\n",
    "print(\"OPS demo dir :\", OPS_DEMO_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "ITEM_TEMPLATES = [\n",
    "    {\n",
    "        \"coupon\": \"NEWBED10\",\n",
    "        \"item_id\": \"M_STAN_F\",\n",
    "        \"item_name\": \"standard full mattress\",\n",
    "        \"item_revenue_in_usd\": \"850.3\",\n",
    "        \"price\": \"849\"\n",
    "    },\n",
    "    {\n",
    "        \"coupon\": \"SLEEP5\",\n",
    "        \"item_id\": \"P_BASIC\",\n",
    "        \"item_name\": \"basic pillow\",\n",
    "        \"item_revenue_in_usd\": \"55.0\",\n",
    "        \"price\": \"59\"\n",
    "    },\n",
    "    {\n",
    "        \"coupon\": None,\n",
    "        \"item_id\": \"B_FRAME_Q\",\n",
    "        \"item_name\": \"queen bed frame\",\n",
    "        \"item_revenue_in_usd\": \"399.0\",\n",
    "        \"price\": \"399\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def random_items():\n",
    "    # returns a JSON string representing a list of dicts (like your example)\n",
    "    n = random.randint(1, 3)\n",
    "    return json.dumps(random.sample(ITEM_TEMPLATES, n))\n",
    "\n",
    "random_items_udf = F.udf(lambda: random_items(), \"string\")\n",
    "\n",
    "def generate_sales_df(start_order_id: int, rows: int):\n",
    "    # transactions_timestamp as string to make it easy to inject 'aaa' in malformed file\n",
    "    return (\n",
    "        spark.range(start_order_id, start_order_id + rows)\n",
    "        .withColumnRenamed(\"id\", \"order_id\")\n",
    "        .withColumn(\"email\", F.concat(F.lit(\"user\"), F.col(\"order_id\"), F.lit(\"@example.com\")))\n",
    "        .withColumn(\n",
    "            \"transactions_timestamp\",\n",
    "            F.date_format(\n",
    "                F.expr(\"timestampadd(SECOND, cast(rand()*86400 as int), current_timestamp())\"),\n",
    "                \"yyyy-MM-dd HH:mm:ss\"\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"total_item_quantity\", (F.rand() * 5 + 1).cast(\"int\"))\n",
    "        .withColumn(\"purchase_revenue_in_usd\", F.round(F.rand() * 1200 + 20, 2))\n",
    "        .withColumn(\"unique_items\", (F.rand() * 3 + 1).cast(\"int\"))\n",
    "        .withColumn(\"items\", random_items_udf())\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"email\",\n",
    "            \"transactions_timestamp\",\n",
    "            \"total_item_quantity\",\n",
    "            \"purchase_revenue_in_usd\",\n",
    "            \"unique_items\",\n",
    "            \"items\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def write_single_csv(df, dest_csv_path: str):\n",
    "    \"\"\"\n",
    "    Writes a single CSV file to dest_csv_path by coalescing to 1 part file,\n",
    "    then moving it to the requested filename.\n",
    "    \"\"\"\n",
    "    tmp_dir = dest_csv_path.replace(\".csv\", \"_tmp\")\n",
    "    (df.coalesce(1)\n",
    "       .write.mode(\"overwrite\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .csv(tmp_dir))\n",
    "\n",
    "    part_file = [f.path for f in dbutils.fs.ls(tmp_dir) if f.name.startswith(\"part-\")][0]\n",
    "    dbutils.fs.rm(dest_csv_path, True)                 # remove if exists\n",
    "    dbutils.fs.mv(part_file, dest_csv_path, True)      # rename part -> desired name\n",
    "    dbutils.fs.rm(tmp_dir, True)                       # cleanup\n",
    "\n",
    "def write_text_file(dest_path: str, text: str):\n",
    "    dbutils.fs.rm(dest_path, True)\n",
    "    dbutils.fs.put(dest_path, text, overwrite=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Create raw/sales-csv/000.csv..003.csv (1500 rows each)\n",
    "# -----------------------------\n",
    "for i in range(NUM_SALES_FILES):\n",
    "    start_id = 1 + i * ROWS_PER_FILE\n",
    "    df_i = generate_sales_df(start_id, ROWS_PER_FILE)\n",
    "    out_path = f\"{RAW_SALES_DIR}/{i:03d}.csv\"\n",
    "    write_single_csv(df_i, out_path)\n",
    "\n",
    "print(\"✅ Sales CSV files created:\", [f.name for f in dbutils.fs.ls(RAW_SALES_DIR)])\n",
    "\n",
    "# We'll base malformed files on 000.csv's content\n",
    "df_000 = spark.read.option(\"header\", \"true\").csv(f\"{RAW_SALES_DIR}/000.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) malformed_example_1_data.csv\n",
    "#    Same as 000.csv, except FIRST ROW transactions_timestamp = 'aaa'\n",
    "# -----------------------------\n",
    "# Make a deterministic \"first row\" using row_number by order_id\n",
    "w = Window.orderBy(F.col(\"order_id\").cast(\"long\"))\n",
    "\n",
    "df_m1 = (df_000\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .withColumn(\n",
    "        \"transactions_timestamp\",\n",
    "        F.when(F.col(\"rn\") == 1, F.lit(\"aaa\")).otherwise(F.col(\"transactions_timestamp\"))\n",
    "    )\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "m1_path = f\"{OPS_DEMO_DIR}/malformed_example_1_data.csv\"\n",
    "write_single_csv(df_m1, m1_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) malformed_example_2_data.csv\n",
    "#    Header has only 6 columns (missing order_id),\n",
    "#    but each record has 7 fields (includes order_id as extra first field)\n",
    "# -----------------------------\n",
    "cols_6 = [\n",
    "    \"email\",\n",
    "    \"transactions_timestamp\",\n",
    "    \"total_item_quantity\",\n",
    "    \"purchase_revenue_in_usd\",\n",
    "    \"unique_items\",\n",
    "    \"items\"\n",
    "]\n",
    "\n",
    "# IMPORTANT: data rows will include order_id + the 6 cols = 7 fields per row\n",
    "cols_7_for_data = [\"order_id\"] + cols_6\n",
    "\n",
    "tmp_dir = f\"{OPS_DEMO_DIR}/m2_tmp\"\n",
    "\n",
    "(df_000.select(*cols_7_for_data)\n",
    "   .coalesce(1)\n",
    "   .write.mode(\"overwrite\")\n",
    "   .option(\"header\", \"false\")\n",
    "   .csv(tmp_dir))\n",
    "\n",
    "part_file = [f.path for f in dbutils.fs.ls(tmp_dir) if f.name.startswith(\"part-\")][0]\n",
    "\n",
    "# Pull the data lines (7-field CSV lines)\n",
    "lines = [r.value for r in spark.read.text(part_file).collect()]\n",
    "\n",
    "# Write a 6-column header (missing order_id)\n",
    "custom_header = \",\".join(cols_6)\n",
    "\n",
    "m2_path = f\"{OPS_DEMO_DIR}/malformed_example_2_data.csv\"\n",
    "dbutils.fs.rm(m2_path, True)\n",
    "dbutils.fs.put(m2_path, custom_header + \"\\n\" + \"\\n\".join(lines) + \"\\n\", overwrite=True)\n",
    "\n",
    "dbutils.fs.rm(tmp_dir, True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Verification\n",
    "# -----------------------------\n",
    "print(\"\\nRAW sales-csv directory listing:\")\n",
    "print([f.name for f in dbutils.fs.ls(RAW_SALES_DIR)])\n",
    "\n",
    "print(\"\\nOPS csv_demo_files directory listing:\")\n",
    "print([f.name for f in dbutils.fs.ls(OPS_DEMO_DIR)])\n",
    "\n",
    "# Row counts for the 4 raw files\n",
    "for i in range(NUM_SALES_FILES):\n",
    "    p = f\"{RAW_SALES_DIR}/{i:03d}.csv\"\n",
    "    c = spark.read.option(\"header\",\"true\").csv(p).count()\n",
    "    print(f\"{i:03d}.csv rows = {c}\")\n",
    "\n",
    "# Quick checks for malformed\n",
    "m1_first_ts = (spark.read.option(\"header\",\"true\").csv(m1_path)\n",
    "               .orderBy(F.col(\"order_id\").cast(\"long\"))\n",
    "               .select(\"transactions_timestamp\").first()[0])\n",
    "print(\"\\nmalformed_example_1_data.csv first transactions_timestamp:\", m1_first_ts)\n",
    "\n",
    "# malformed2: read as text to confirm header has 6 cols\n",
    "m2_head = dbutils.fs.head(m2_path, 200)\n",
    "print(\"\\nmalformed_example_2_data.csv header line:\")\n",
    "print(m2_head.splitlines()[0])\n",
    "\n",
    "print(\"\\n✅ Setup complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab-setup-04",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
