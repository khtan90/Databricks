{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "234f8620-e767-4854-b440-196233714ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Introduction\n",
    "- We will explore how to ingest JSON files and perform foundational JSON speciifc transformations during ingestion\n",
    "- We will use simulated Kafka event data\n",
    "\n",
    "### Learning Objectives\n",
    "- Ingest raw JSON into Unity Catalog using CTAS and read_files()\n",
    "- Apply techniques to flatten JSON string columns\n",
    "- Understand difference between explode() and explode_outer()\n",
    "- Introduce capabilities and use cases of VARAINT data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34e2593a-c4ad-4fcc-8081-658870c8f738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup\n",
    "- run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb125c5-78eb-4089-a1c4-3b840d3a0cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a15f39-4020-4f74-95fd-5a87fb0a123b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explore the data\n",
    "- There should be 6 keys.\n",
    "- key and value fields are encoded in base64.(Encoding scheme that converts binary data into a readable ASCII string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b7564e-bed5-402e-8670-5bde9491b082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- View current catalog and schema you are using\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2516837-601f-4a40-a583-a8f9c51fe672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Verify that there are 11 JSON files in our volume\n",
    "LIST '/Volumes/workspace/data_engineering_labs_00/v01/raw/events-kafka/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77388bb4-f287-454e-8a68-b0814a217a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Using text helps us get a quick preview\n",
    "SELECT * \n",
    "FROM text.`/Volumes/workspace/data_engineering_labs_00/v01/raw/events-kafka/`\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d2b7752-0d57-4081-b801-19a4a9ebd262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Read the files into tablular format using read_files()\n",
    "- Lets store the raw data into a table called `kafka_events_bronze_raw`\n",
    "- Then lets decode them into a second layer bronze table `kafka_events_bronze_decoded` (We could always just have 1 layer, its up to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea65315-3622-4df9-8316-00cfa2953f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Explore how it looks like in Tabular form\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "  \"/Volumes/workspace/data_engineering_labs_00/v01/raw/events-kafka/\",\n",
    "  format => \"json\"\n",
    ")\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1dbfd7b-11c9-480f-85b7-4aef673d279e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- DROP TABLE IF Exists\n",
    "DROP TABLE IF EXISTS workspace.data_engineering_labs_00.kafka_events_bronze_raw;\n",
    "\n",
    "-- Create the delta bronze table\n",
    "CREATE TABLE data_engineering_labs_00.kafka_events_bronze_raw AS\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "  \"/Volumes/workspace/data_engineering_labs_00/v01/raw/events-kafka/\",\n",
    "  format => \"json\"\n",
    ");\n",
    "\n",
    "-- Display\n",
    "SELECT * FROM data_engineering_labs_00.kafka_events_bronze_raw\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc134d04-bcc4-4bef-947a-5a79f951a227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Decode base64 strings for the bronze table\n",
    "- Use `unbase64()` function\n",
    "- Note that the decoded values are now binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac02c11a-02e3-4f59-b985-08e98d8378b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "key as encoded_key,\n",
    "unbase64(key) as decoded_key,\n",
    "value as encoded_value,\n",
    "unbase64(value) as decoded_value\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_raw\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64dc81ef-d800-4d61-b710-645b1255849f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- We now use `cast()` to convert binary columns to Strings, making them type strign and readable\n",
    "- decoded value column ius now a JSON -formatted string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae85b59-c0ab-4b62-957b-2b85b001fe10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "key as encoded_key,\n",
    "cast(unbase64(key) as string) as decoded_key,\n",
    "value as encoded_value,\n",
    "cast(unbase64(value)as string ) as decoded_value\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_raw\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f6fdc3d-8e9b-4d09-b375-2ff9d309bf92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Now we put them all tgt and create the 2nd level bronze table `kafka_events_bronze_decoded`\n",
    "- ignore if decoded key look strange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b8daba-afc1-46c1-8240-92e1b485713b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE data_engineering_labs_00.kafka_events_bronze_decoded AS \n",
    "SELECT \n",
    "  cast(unbase64(key) as string) as decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  cast(unbase64(value)as string ) as decoded_value\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_raw;\n",
    "\n",
    "--View\n",
    "SELECT * FROM data_engineering_labs_00.kafka_events_bronze_decoded\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d85d4a7-f511-4255-b662-ccadc46a4394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2a. Working with JSON-formatted strings in a table (flatten it using `:`)\n",
    "\n",
    "Benefits\n",
    "- Simple: Easy to implment and store JSON as plain text\n",
    "- Flexible: Can hold any JSON structure without schema constraints\n",
    "\n",
    "Considerations:\n",
    "- Performance: STRING columns are slower when querying and porocesing data\n",
    "- No Schema: Lack of a defined schema for STRING column can lead to data integrity issus\n",
    "- Complex to query: Requitres additional code to parse and retrieve data\n",
    "\n",
    "How to:\n",
    " Query using `column_name : extraction_path` using `.` or `[]` when accessing nested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba51f40-8187-403c-9b74-503f2d11b30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "decoded_value:device,\n",
    "decoded_value:traffic_source,\n",
    "decoded_value:geo, --contains anotheer JSON formatted string\n",
    "decoded_value:items --contains a nested array of JSON fomatted strings\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_decoded\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0760da-7969-456b-99f9-e1029f6782da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create the Table `kafka_events_bronze_string_flattened`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b29f110-db56-4233-a5e3-b9ba4979e8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE data_engineering_labs_00.kafka_events_bronze_string_flattened AS \n",
    "SELECT\n",
    "decoded_key,\n",
    "offset,\n",
    "partition,\n",
    "timestamp,\n",
    "topic,\n",
    "decoded_value:device,\n",
    "decoded_value:traffic_source,\n",
    "decoded_value:geo, --contains anotheer JSON formatted string\n",
    "decoded_value:items --contains a nested array of JSON fomatted strings\n",
    "\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_decoded;\n",
    "\n",
    "--View\n",
    "SELECT * FROM data_engineering_labs_00.kafka_events_bronze_string_flattened\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "414377b3-f096-4797-8115-9d51ce511967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2b. Working with JSON-formatted strings in a table (flatten it using STRUCT conversion)\n",
    "\n",
    "Benefits\n",
    "- Schema enforcement: STRUCT columns dfin and enforce a schema, helping maintain data integirty\n",
    "-Improved Performanc = STRUCTs are more efficient for querying and procssing then strings\n",
    "\n",
    "Considerations\n",
    "- Schema enforcment: As schema is enforced, issues may arise if JSON structure change over timee\n",
    "- Reduced flexibility: Data must consistentlky match the defined schema, less room for structural variation\n",
    "\n",
    "How to: (2 steps)\n",
    "1. Get schema of string using `schema_of_json`, paste an example schema there and it will help us retrieve it as type STRUCT\n",
    "2. Apply schema to json formatted column by copy and paste the output from step 1 into `from_json()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0ac008-88fb-4258-a7d4-f5bd91bc0d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- This will return the schema as a type STRUCT\n",
    "SELECT schema_of_json(\n",
    "'{\"device\":\"Windows\",\"ecommerce\":{\"purchase_revenue_in_usd\":1195.0,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"main\",\"event_timestamp\":176757862507185181,\"geo\":{\"city\":\"San Francisco\",\"state\":\"CA\"},\"items\":[{\"coupon\":null,\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1195.0,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"google\",\"user_first_touch_timestamp\":1762394625071,\"user_id\":\"UA1043321819\"}')\n",
    "AS SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4bc1ec-e316-407f-97f9-31c1c1071beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Final table\n",
    "CREATE OR REPLACE TABLE data_engineering_labs_00.kafka_events_bronze_string_struct AS \n",
    "SELECT * EXCEPT (decoded_value),\n",
    "from_json(decoded_value, \n",
    "'STRUCT<device: STRING, ecommerce: STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name: STRING, event_timestamp: BIGINT, geo: STRUCT<city: STRING, state: STRING>, items: ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source: STRING, user_first_touch_timestamp: BIGINT, user_id: STRING>'\n",
    ") AS Value\n",
    "\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_decoded;\n",
    "\n",
    "--View\n",
    "SELECT * FROM data_engineering_labs_00.kafka_events_bronze_string_struct\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f61a45f7-3566-47a4-9883-a08d95c44994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- We can then use `.` to query the STRUCT column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab15e52-e658-4bd0-94e1-86d870736ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT decoded_key,\n",
    "value.device as dvice,\n",
    "value.geo.city as city,\n",
    "value.items as items,\n",
    "array_size(items) as number_of_elemnts_in_array\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_string_struct\n",
    "ORDER BY number_of_elemnts_in_array DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6766673e-3044-44ac-b867-6c8953b3b6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Note the items columns\n",
    "  - It is an array of Elments.\n",
    "  - We can use `explode` to transofrm each element into its own sparate row.\n",
    "  - If NULL, no rows are produced. If we want to return rows when there are nulls, use `explode_outer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1efff8-bf40-48aa-8e12-21e8d0d160a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE data_engineering_labs_00.kafka_events_bronze_explode_array AS \n",
    "SELECT decoded_key,\n",
    "array_size(value.items) AS number_elements_in_array,\n",
    "explode(value.items) as item_in_array, --individual item of the array\n",
    "value.items --original data for comparision\n",
    "\n",
    "\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_string_struct\n",
    "ORDER BY number_elements_in_array DESC;\n",
    "\n",
    "--View\n",
    "SELECT * FROM data_engineering_labs_00.kafka_events_bronze_explode_array\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a79b7af-8b3f-4c82-bf97-ca5d07f9b00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Using VARIANT\n",
    "Beenefits:\n",
    "- Open source\n",
    "- Flexible: No strict schma, can put any kind of semi structuree data\n",
    "- Performant: Improvd prformancw over exisitng methods\n",
    "\n",
    "How to:\n",
    "- Use `parse_json` function to return a VARIANT value\n",
    "- We can pars variant data `:` to create desired table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b785b553-7e92-4b4b-9240-7c69e638fade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Lets use the decodd table for a start\n",
    "SELECT *\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_decoded\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "352a2a10-9af2-4f89-ad8c-35e5fce803f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that after running cell, json_variant_value is of type variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029ce09d-93e3-459c-b1b7-74ee4d6ac08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE data_engineering_labs_00.kafka_events_bronze_variant AS \n",
    "SELECT \n",
    "  decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  parse_json(decoded_value) AS json_variant_value\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_decoded;\n",
    "\n",
    "--View\n",
    "SELECT * FROM data_engineering_labs_00.kafka_events_bronze_variant\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b7cfdc-a271-4b7c-addc-0ddda96de4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- USe : to create desired tabl\n",
    "\n",
    "SELECT\n",
    "json_variant_value,\n",
    "json_variant_value:device :: STRING, -- parse device as string\n",
    "json_variant_value:items\n",
    "FROM data_engineering_labs_00.kafka_events_bronze_variant\n",
    "LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 Ingesting JSON Files with Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
