{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4fc9f84-b45f-4e89-b60d-1d263e5bd534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lesson\n",
    "- In this lesson, we will create a streaming table to incrementally ingest files from a volume using Auto Loader with SQL\n",
    "\n",
    "### Learning Objectives\n",
    "- Objective 1: Create streaming tables in Databricks SQL for incremental data ingestion\n",
    "- Objective 2: Refresh streaming tables using the REFRESH statement\n",
    "\n",
    "### Recommendation\n",
    "- The CREATE STREAMING TABLE SQL command is the recommended alternative to the legacy COPY INTO SQL command for incremental ingestion form cloud object storage.\n",
    "A streaming table is a table that is registered to the UNity Catalog with extra support for streaming/incremental data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c925210-02ac-44cd-90b8-2bdc0fa7909d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 01 Run the Command below to setup the data for the lab\n",
    "- 2 folders will be created in workspace.data_engineering_labs.v01, with csv files in them.\n",
    "  - csv_files_autoloader_source\n",
    "    - 000.csv (3150) rows\n",
    "  - csv_files_autoloader_staging\n",
    "    - 001.csv  (1000) rows\n",
    "    - 002.csv (2000) rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6f86d2-d4ad-453b-85f3-5a704d217737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b296fd1-e620-4aef-94dd-ceea26eb7adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 02 Run the query below to view the data in csv_files_autoloader_source.\n",
    "- Note that it has 3150 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924779de-6734-4242-a347-6f3d6d89b4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM read_files(\n",
    "  '/Volumes/workspace/data_engineering_labs_00/v01/csv_files_autoloader_source/',\n",
    "  format => 'CSV',\n",
    "  sep => ',',\n",
    "  header => 'true'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dbce34f-c3e9-4686-ae1f-e19640e424d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 03 Objective 1: Create a Streaming Table in Databricks SQL\n",
    "- The code will create a streaming table that incrementally ingests new data every week\n",
    "- The incremental batch ingestion will automatically detect new records in the data source and ignores records that have already been ingested.\n",
    "\n",
    "* Note: Seems like there is a change as compared to tutorial in databricks\n",
    "  -  Tutorial says that a pipeline will be automatically created.\n",
    "  - However, now after running code, it seems like we must schedule a pipeline.\n",
    "    - This is in line with what we have learn with LSDP(which is pretty new)\n",
    "  - Solution is to create a Pipeline and paste the below cell there to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2ce5ac-953a-443e-a610-89daa79be7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creates the Streaming table\n",
    "-- STREAM read_files enables autoloader\n",
    "\n",
    "CREATE OR REFRESH STREAMING TABLE workspace.data_engineering_labs_00.sql_csv_autoloader\n",
    "SCHEDULE EVERY 1 WEEK AS\n",
    "\n",
    "SELECT * \n",
    "FROM\n",
    "STREAM read_files('/Volumes/workspace/data_engineering_labs_00/v01/csv_files_autoloader_source/',\n",
    "format => 'CSV',\n",
    "sep = ',',\n",
    "header => true\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7021c53-ec23-4480-953d-63c4475b9221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- View the table after running in pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156582bc-cd4e-4113-ac05-7c57e37881f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM workspace.data_engineering_labs_00.sql_csv_autoloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2565c1b-a92d-4ee3-801b-e4d8a07df83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- We can see that the table is a streaming table\n",
    "- My refresh schedule seems to be manual and not 1 week, (Maybe as i did not set it to be that way in the pipeline, despite the code)\n",
    "- Again, we should use LSDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59277f1d-9c6e-44e8-a89f-1e962cc23db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED workspace.data_engineering_labs_00.sql_csv_autoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab7dbc96-0453-4885-aaf9-280a655eddf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY workspace.data_engineering_labs_00.sql_csv_autoloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ca586a6-5cea-40e1-ae69-4d7e33ad5a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 04 Objective 2: Try to run Refresh\n",
    "- Download a csv file from staging folder  \n",
    "- Upload it to the source folder\n",
    "- Manually refresh the streaming table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1637bdc8-93a7-4f70-b8c1-2518717cf0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "REFRESH STREAMING TABLE workspace.data_engineering_labs_00.sql_csv_autoloader"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6573044843167707,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02 Create Streaming Tables with SQL using Auto Loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
