{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab23a2e-168a-4e58-b6a7-15cfadcb7254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning Objectives\n",
    "- Objective 1: Use the CTAS statement with `read_files()` to ingest Parquet files into a Delta table\n",
    "- Objective 2: Use COPY INTO to incrementally load Parquet files from cloud object storage into a Delta table\n",
    "    - Usage of `COPY_OPTIONS ('mergeSchema' = 'true')` to handle schema changes\n",
    "- Objective 3: Review of Managed vs External Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7621f671-b19d-4e6a-bc1b-ab6b44733e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set Up the Lab and Confirm our default catalog and schema\n",
    "- `%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-01 `\n",
    "    - Include argument `run_id` = `02`, `03`, `04`...etc for multiple environments\n",
    "    - This will create a catalog.schema called workspace.data_engineering _labs_00\n",
    "    - There will be 4 parquet files totalling 10,000 records created under V01/raw/users-historical\n",
    "- Use `current_catalog()` and `current_schema()` to confirm it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "73851aee-c2c8-4736-91bc-d2141720eca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1911fa1e-4ea3-4519-b4a3-8a861238f96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dd2e636-2add-4381-a005-899029cbe5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explore the Data Files\n",
    "1. We will create an table from Parquet files stored in our volume\n",
    "`/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/`\n",
    "\n",
    "2. We can use the `dbutils.fs.ls` statement to view the files in our volume.\n",
    "\n",
    "3. We can also query the parquet files by path to quickly preview the files in table form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124f72eb-0444-4de8-9975-4dcebbd31705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/\"\n",
    "files = dbutils.fs.ls(path)\n",
    "display([(f.name, f.size) for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5837aef9-03eb-4271-9d34-5d6d67594cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * \n",
    "FROM parquet.`/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/` -- use backticks for filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9747456b-5d6b-4039-ada1-ee99415c91e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch Ingestion using CTAS with the `read_files()` Function\n",
    "- Note: A `_rescued_data` column is automatically included to capture any data that does not match inferred schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e55ad22-21c0-4313-a6dc-b4ab03e0614f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- READ files\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/',\n",
    "  format => 'parquet'\n",
    ")\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabde3f0-9827-4be4-a7f1-ff00ac46e874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- We will use a CTAS statement to create the table `historical_users_bronze_ctas_rf`\n",
    "- Table type is Delta by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc85246-108d-49d6-8e7c-b995c284702e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Drop table \n",
    "DROP TABLE IF EXISTS historical_users_bronze_ctas_rf;\n",
    "\n",
    "-- Create Delta Table\n",
    "CREATE TABLE historical_users_bronze_ctas_rf AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  '/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/',\n",
    "  format => 'parquet'\n",
    ");\n",
    "\n",
    "--Preview\n",
    "SELECT * \n",
    "FROM historical_users_bronze_ctas_rf\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "202b68bb-2f62-4d27-b224-68b799faaaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can run `DESCRIBE TABLE EXTENDED` to view column names, data types and additional table metadata\n",
    "- Note that:\n",
    "    - the table was created in our catalog `workspace`\n",
    "    - schema = `data_engineering_labs_00`\n",
    "    - Table Type = `MANAGED`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fc8542-dc1f-4a42-a92f-4a4a162cf1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "DESCRIBE TABLE EXTENDED historical_users_bronze_ctas_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4c01005-2d7c-4fd9-a14e-6f4ec2ce6c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Managed vs External tables\n",
    "1. Managed\n",
    "  - DB manages both data and metadata\n",
    "  - Data is stored in DB managed storage\n",
    "  - Dropping table also deletes data\n",
    "2. External\n",
    "  - DB only manages table metadata\n",
    "  - Dropping table does not delete data\n",
    "  - Supports multiple formats including Delta Lake\n",
    "  - Ideal for sharing data across platforms or using. external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fff0f45-8f92-4cfa-b4d7-ff57031da61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Batch Python Ingestion \n",
    "- This code uses python as an alteernative to SQL to ingest the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e8f9bd-a9ee-4af9-9138-e751382bd2c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/\"\n",
    "\n",
    "# 1.  Read the parquet files from the volumn into a Spark dataframe\n",
    "df = spark.read\\\n",
    "    .format(\"parquet\")\\\n",
    "    .load(file_path)\n",
    "\n",
    "# 2. Write df to a delta table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"workspace.data_engineering_labs_00.historical_users_bronze_python\")\n",
    "\n",
    "# 3. read and display\n",
    "users_bronze_table = spark.table(\"workspace.data_engineering_labs_00.historical_users_bronze_python\")\n",
    "display(users_bronze_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e4624bc-df8f-45be-a0d4-9ec2a353d400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Incremental Data Ingestion with `COPY INTO`\n",
    "- Exisitng files are tracked and will be skipped\n",
    "  - Useful when we need to load data into an exisitng Delta table\n",
    "- Note that moving ahead, `COPY INTO` is considered legacy and Auto Loader is recommneded instead for incremntal ingestion.\n",
    "- We will use the same set of files to create our Bronze table again.\n",
    "- There will also be 2 examples\n",
    "  1. Create Table with Schema then handle Common Schema Mismatch Error\n",
    "  2. Create Table without Schema then Preemptively Handling schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "949b61cc-d284-4726-907a-a4e62bf43496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 1: Create Table with Schema then handle Common Schema Mismatch Error\n",
    "\n",
    "- Empty table `historical_users_bronze_ci` created with a defined schema for columns\n",
    "  -  `user_id`\n",
    "  - `user_first_touch_timestamp` \n",
    "\n",
    "- However, the Parquet files has 3 columns. \n",
    "  -  `user_id`\n",
    "  - `user_first_touch_timestamp` \n",
    "  - `email`\n",
    "\n",
    "The difference in schema cause the error. \n",
    "- We fix the error by adding `COPY_OPTIONS` with `mergeSchema` = True\n",
    "- This allows the schema to evolve based on incoming data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f15c3dc-ff0e-4d6e-945c-79271a7d570f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- DROP Table \n",
    "DROP TABLE IF EXISTS historical_users_bronze_ci;\n",
    "\n",
    "-- Create empty table with 2 columns only\n",
    "CREATE TABLE historical_users_bronze_ci (\n",
    "  user_id LONG,\n",
    "  user_first_touch_timestamp BIGINT\n",
    ");\n",
    "\n",
    "-- USE COPY INTO to populate Delta table\n",
    "COPY INTO historical_users_bronze_ci\n",
    "FROM '/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/'\n",
    "FILEFORMAT = PARQUET;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580cfaa8-189c-4397-9c96-0b74ad970b11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- USE COPY INTO to populate Delta table with mergeSchema = True\n",
    "COPY INTO historical_users_bronze_ci\n",
    "FROM '/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/'\n",
    "FILEFORMAT = PARQUET\n",
    "COPY_OPTIONS ('mergeSchema' = 'true'); -- merges schema of each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0bebb2-1110-402a-a236-a044c007829e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Example 2: Create Table without Schema then Preemptively Handling schema evolution\n",
    "- We can also create atable without schema, then adding `COPY_OPTIONS` with `mergeSchema` = True like before\n",
    "- This will enable schema evolution for the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9681ac7-b7d3-477f-bf13-b621342bc935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DROP Table \n",
    "DROP TABLE IF EXISTS historical_users_bronze_ci_no_schema;\n",
    "\n",
    "-- Create empty table with 2 columns only\n",
    "CREATE TABLE historical_users_bronze_ci_no_schema;\n",
    "\n",
    "-- USE COPY INTO to populate Delta table\n",
    "COPY INTO historical_users_bronze_ci_no_schema\n",
    "FROM '/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/'\n",
    "FILEFORMAT = PARQUET\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "010aa6eb-4240-4a27-bd81-57407c2d3bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Note that due to incremnetal batch, next run will creeatee no new rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58497b6d-526f-42bd-abe6-6386da983b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- USE COPY INTO to populate Delta table\n",
    "COPY INTO historical_users_bronze_ci_no_schema\n",
    "FROM '/Volumes/workspace/data_engineering_labs_00/v01/raw/users-historical/'\n",
    "FILEFORMAT = PARQUET\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4894886312681012,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01 Data Ingestion with CREATE TABLE AS and COPY INTO",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
