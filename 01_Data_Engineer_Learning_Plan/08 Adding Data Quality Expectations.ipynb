{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30eddfe7-2f06-44ee-9315-ec09a2f7982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning Objectives\n",
    "- This lab continues on from lab \"07 Develop a Simple Pipeline using LSDP\"\n",
    "- We will attempt to add data quality expectations to the streaming tables.\n",
    "- It will come in 3 flavours\n",
    "\n",
    "  1. WARN\n",
    "      e.g. - `CONSTRAINT valid_notification EXPECT (notifications IN ('Y', 'N'))`\n",
    "  2. DROP\n",
    "      e.g. - `CONSTRAINT valid_date EXPECT (order_timestamp > \"2021-01-01\") ON VIOLATION DROP ROW`\n",
    "  3. FAIL\n",
    "     e.g. - `CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7846a50-74be-4563-8e2c-47cce00dc252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set Up\n",
    "- Run `%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-06` if not already done from lab 07\n",
    "- This should create 4 datasets\n",
    "  1. `orders/00.json` -> 174 rows\n",
    "  2. `status/00.json` -> 5000 rows\n",
    "  3. `customers/00.json` --> 1000 rows\n",
    "  4. `customers_new01.json` --> 23 rows\n",
    "- In this lab, we will only focus on the `orders` dataset. The rest of the datasets will be used in the next few labs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4bcf18-1230-4838-88ec-6e1d29da8d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c702571f-f293-4ccd-ab4c-e67e6e48cd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Steps: \n",
    "1. Select folder where you want to store your pipeline\n",
    "2. Select `Create ETL Pipeline` \n",
    "  - This is a UI set up to define your souce folder where pipeline will run.\n",
    "  - We can create notebooks here too to run code (Create `orders_pipeline.sql`) in this step. (Code below)\n",
    "    - This code will look similar to lab 7, with the addition of constraints\n",
    "  - In the UI, click settings to change common settings\n",
    "    - Under config, we will put key: source, value : `/Volumes/workspace/data_engineering_labs_00/v01` to parameterize the volume location.\n",
    "3. We can click dry run when ready: This will help to check for errors, without creation of actual tables\n",
    "4. We can run pipeline with full table refresh(can be dangerous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1f96ba-f541-423c-9fbe-6181e47096a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example \n",
    " - Sample `orders_pipeline.sql` code with constraints\n",
    " - Do not run it here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f5ccf3a-ef28-4427-ac79-699d226ecb81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explaination\n",
    "\n",
    "- We checked for a 'Yes' or 'No' in notifications column, and only notify for different values. As our data is all 'Y' or 'N', the expectation fails, notifies us, but still writes results\n",
    "- We checked for order_timestamp > \"2022-01-01\", and drop for different values. This will end up dropping 53 records.\n",
    "- We also wrote a failure expectat6ions on customer_id. This passess all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7db29b-d488-45a2-a09d-fb4b9040fa48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- 1. Create a bronze streaming table from our volume. \n",
    "CREATE OR REFRESH STREAMING TABLE workspace.data_engineering_labs_00.bronze_demo_expectations\n",
    "AS\n",
    "SELECT \n",
    "*, \n",
    "current_timestamp() AS processing_time,\n",
    "_metadata.file_name AS source_file\n",
    "FROM \n",
    "STREAM read_files(\n",
    "  \"${source}/orders\", -- source config variable set in pipeline settings\n",
    "  format => 'JSON'\n",
    ");\n",
    "\n",
    "\n",
    "-- 2. Create a silver streaming table from our bronze table, with a transform to convert the timestamp\n",
    "CREATE OR REFRESH STREAMING TABLE workspace.data_engineering_labs_00.silver_demo_expectations\n",
    "\n",
    "-- Add the expectations\n",
    "(\n",
    "CONSTRAINT valid_notification EXPECT (notifications IN ('Yes', 'No')), -- Check for a Yes or No in notifications column\n",
    "CONSTRAINT valid_date EXPECT (order_timestamp > \"2022-01-01\") ON VIOLATION DROP ROW, --drop row if not valid date\n",
    "CONSTRAINT valid_id EXPECT (customer_id IS NOT NULL) ON VIOLATION FAIL UPDATE -- Fail pipeline if null\n",
    ")\n",
    "\n",
    "\n",
    "AS\n",
    "SELECT \n",
    "order_id,\n",
    "timestamp(order_timestamp) AS order_timestamp,\n",
    "customer_id,\n",
    "notifications\n",
    "FROM \n",
    "STREAM bronze_demo_expectations;\n",
    "\n",
    "-- 3. Create a materialised view from our silver table\n",
    "CREATE OR REFRESH MATERIALIZED VIEW workspace.data_engineering_labs_00.gold_orders_by_date_demo_expectations\n",
    "AS\n",
    "SELECT \n",
    "date(order_timestamp) AS order_date,\n",
    "count(*) AS total_daily_orders\n",
    "FROM \n",
    "silver_demo_expectations\n",
    "GROUP BY date(order_timestamp);"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08 Adding Data Quality Expectations",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
