{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30eddfe7-2f06-44ee-9315-ec09a2f7982f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Learning Objectives\n",
    "- This lab is a demonstration of setting up a simple ETL pipeline, before the use of Lakeflow Spark Declarative Pipelines.\n",
    "  1. We will use the UI to Create a folder where our Pipeline code is stored\n",
    "  2. We will run a Pipeline that calls a notebook called `orders_pipeline.sql`\n",
    "  3. We will demonstrate the use of config settings to parameterize sql code.\n",
    "  3. The pipeline will ingest from orders/oo.json to create a bronze streaming table -> silver streaming table -> gold materialzed view.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7846a50-74be-4563-8e2c-47cce00dc252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set Up\n",
    "- Run `%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-06`\n",
    "- This should create 4 datasets\n",
    "  1. `orders/00.json` -> 174 rows\n",
    "  2. `status/00.json` -> 5000 rows\n",
    "  3. `customers/00.json` --> 1000 rows\n",
    "  4. `customers_new01.json` --> 23 rows\n",
    "- In this lab, we will only focus on the `orders` dataset. The rest of the datasets will be used in the next 2 labs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce4bcf18-1230-4838-88ec-6e1d29da8d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../01_Data_Engineer_Learning_Plan/Lab-Setup/lab-setup-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c702571f-f293-4ccd-ab4c-e67e6e48cd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Steps: \n",
    "1. Select folder where you want to store your pipeline\n",
    "2. Select `Create ETL Pipeline` \n",
    "  - This is a UI set up to define your souce folder where pieplien will run.\n",
    "  - We can create notebooks here too to run code (Create `orders_pipeline.sql`) in this step. (Code below)\n",
    "  - In the UI, click settings to change common settings\n",
    "    - Under config, we will put key: source, value : `/Volumes/workspace/data_engineering_labs_00/v01` to parameterize the volume location.\n",
    "3. We can click dry run when ready: This will help to chekc for errros, without creation of actual tables\n",
    "4. We can run pipeline with full table refresh(can be dangerous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1f96ba-f541-423c-9fbe-6181e47096a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example \n",
    " - Sample `orders_pipeline.sql` code \n",
    " - Do not run it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7db29b-d488-45a2-a09d-fb4b9040fa48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- 1. Create a bronze streaming table from our volume. \n",
    "CREATE OR REFRESH STREAMING TABLE workspace.data_engineering_labs_00.bronze_demo\n",
    "AS\n",
    "SELECT \n",
    "*, \n",
    "current_timestamp() AS processing_time,\n",
    "_metadata.file_name AS source_file\n",
    "FROM \n",
    "STREAM read_files(\n",
    "  \"${source}/orders\", -- source config variable set in pipeline settings\n",
    "  format => 'JSON'\n",
    ");\n",
    "\n",
    "\n",
    "-- 2. Create a silver streaming table from our bronze table, with a transform to convert the timestamp\n",
    "CREATE OR REFRESH STREAMING TABLE workspace.data_engineering_labs_00.silver_demo\n",
    "AS\n",
    "SELECT \n",
    "order_id,\n",
    "timestamp(order_timestamp) AS order_timestamp,\n",
    "customer_id,\n",
    "notifications\n",
    "FROM \n",
    "STREAM bronze_demo;\n",
    "\n",
    "-- 3. Create a materialised view from our silver table\n",
    "CREATE OR REFRESH MATERIALIZED VIEW workspace.data_engineering_labs_00.gold_orders_by_date_demo\n",
    "AS\n",
    "SELECT \n",
    "date(order_timestamp) AS order_date,\n",
    "count(*) AS total_daily_orders\n",
    "FROM \n",
    "silver_demo\n",
    "GROUP BY date(order_timestamp);"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07 Develop a Simple Pipeline using LSDP",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
